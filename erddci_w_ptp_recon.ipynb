{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# ERDDCI with Prompt-to-Prompt(Stable Diffusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "import os, gc\n",
    "import abc\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import lpips\n",
    "from skimage.metrics import structural_similarity as SSIM\n",
    "from skimage.metrics import peak_signal_noise_ratio as PSNR\n",
    "\n",
    "import ptp_utils\n",
    "import seq_aligner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For loading the Stable Diffusion using Diffusers, follow the instuctions https://huggingface.co/blog/stable_diffusion and update `MY_TOKEN` with your token.\n",
    "Set `LOW_RESOURCE` to `True` for running on 12GB GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MY_TOKEN = ''\n",
    "LOW_RESOURCE = False\n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 1.0\n",
    "MAX_NUM_WORDS = 77 * 4\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab397bb478f4521a189d85b651c5c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djm/ERDDCI/.venv/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:223: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.33.1\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": false,\n",
      "  \"clip_sample_range\": 1.0,\n",
      "  \"dynamic_thresholding_ratio\": 0.995,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"rescale_betas_zero_snr\": false,\n",
      "  \"sample_max_value\": 1.0,\n",
      "  \"set_alpha_to_one\": false,\n",
      "  \"steps_offset\": 0,\n",
      "  \"thresholding\": false,\n",
      "  \"timestep_spacing\": \"leading\",\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    ")\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", local_files_only=True, scheduler=scheduler\n",
    ").to(device)\n",
    "try:\n",
    "    ldm_stable.disable_xformers_memory_efficient_attention()\n",
    "except AttributeError:\n",
    "    print(\"Attribute disable_xformers_memory_efficient_attention() is missing\")\n",
    "tokenizer = ldm_stable.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Prompt-to-Prompt Attnetion Controllers\n",
    "\n",
    "The warnings and errors in this P2P code section are non-critical - please proceed with execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBlend:\n",
    "    def __call__(self, x_t, attention_store):\n",
    "        k = 1\n",
    "        maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "        maps = [\n",
    "            item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS)\n",
    "            for item in maps\n",
    "        ]\n",
    "        maps = torch.cat(maps, dim=1)\n",
    "        maps = (maps * self.alpha_layers).sum(-1).mean(1)\n",
    "        mask = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 + 1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(mask, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        mask = mask.gt(self.threshold)\n",
    "        mask = (mask[:1] + mask[1:]).float()\n",
    "        x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "\n",
    "    def __init__(self, prompts: List[str], words: [List[List[str]]], threshold=0.3):\n",
    "        alpha_layers = torch.zeros(len(prompts), 1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        self.alpha_layers = alpha_layers.to(device)\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class AttentionControl(abc.ABC):\n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "\n",
    "    def between_steps(self):\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                h = attn.shape[0]\n",
    "                attn[h // 2 :] = self.forward(attn[h // 2 :], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "\n",
    "class EmptyControl(AttentionControl):\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "\n",
    "\n",
    "class AttentionStore(AttentionControl):\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\n",
    "            \"down_cross\": [],\n",
    "            \"mid_cross\": [],\n",
    "            \"up_cross\": [],\n",
    "            \"down_self\": [],\n",
    "            \"mid_self\": [],\n",
    "            \"up_self\": [],\n",
    "        }\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32**2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {\n",
    "            key: [item / self.cur_step for item in self.attention_store[key]]\n",
    "            for key in self.attention_store\n",
    "        }\n",
    "        return average_attention\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "\n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "\n",
    "    def replace_self_attention(self, attn_base, att_replace):\n",
    "        if att_replace.shape[2] <= 16**2:\n",
    "            return attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "        else:\n",
    "            return att_replace\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (\n",
    "            self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]\n",
    "        ):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
    "                attn_repalce_new = (\n",
    "                    self.replace_cross_attention(attn_base, attn_repalce) * alpha_words\n",
    "                    + (1 - alpha_words) * attn_repalce\n",
    "                )\n",
    "                attn[1:] = attn_repalce_new\n",
    "            else:\n",
    "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: Union[\n",
    "            float, Tuple[float, float], Dict[str, Tuple[float, float]]\n",
    "        ],\n",
    "        self_replace_steps: Union[float, Tuple[float, float]],\n",
    "        local_blend: Optional[LocalBlend],\n",
    "    ):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(\n",
    "            prompts, num_steps, cross_replace_steps, tokenizer\n",
    "        ).to(device)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = (\n",
    "            int(num_steps * self_replace_steps[0]),\n",
    "            int(num_steps * self_replace_steps[1]),\n",
    "        )\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "\n",
    "class AttentionReplace(AttentionControlEdit):\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        return torch.einsum(\"hpw,bwn->bhpn\", attn_base, self.mapper)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: float,\n",
    "        self_replace_steps: float,\n",
    "        local_blend: Optional[LocalBlend] = None,\n",
    "    ):\n",
    "        super(AttentionReplace, self).__init__(\n",
    "            prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend\n",
    "        )\n",
    "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
    "\n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: float,\n",
    "        self_replace_steps: float,\n",
    "        local_blend: Optional[LocalBlend] = None,\n",
    "    ):\n",
    "        super(AttentionRefine, self).__init__(\n",
    "            prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend\n",
    "        )\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "\n",
    "class AttentionReweight(AttentionControlEdit):\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        if self.prev_controller is not None:\n",
    "            attn_base = self.prev_controller.replace_cross_attention(\n",
    "                attn_base, att_replace\n",
    "            )\n",
    "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: float,\n",
    "        self_replace_steps: float,\n",
    "        equalizer,\n",
    "        local_blend: Optional[LocalBlend] = None,\n",
    "        controller: Optional[AttentionControlEdit] = None,\n",
    "    ):\n",
    "        super(AttentionReweight, self).__init__(\n",
    "            prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend\n",
    "        )\n",
    "        self.equalizer = equalizer.to(device)\n",
    "        self.prev_controller = controller\n",
    "\n",
    "\n",
    "def get_equalizer(\n",
    "    text: str,\n",
    "    word_select: Union[int, Tuple[int, ...]],\n",
    "    values: Union[List[float], Tuple[float, ...]],\n",
    "):\n",
    "    if type(word_select) is int or type(word_select) is str:\n",
    "        word_select = (word_select,)\n",
    "    equalizer = torch.ones(1, 77)\n",
    "\n",
    "    for word, val in zip(word_select, values):\n",
    "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
    "        equalizer[:, inds] = val\n",
    "    return equalizer\n",
    "\n",
    "\n",
    "def aggregate_attention(\n",
    "    attention_store: AttentionStore,\n",
    "    res: int,\n",
    "    from_where: List[str],\n",
    "    is_cross: bool,\n",
    "    select: int,\n",
    "):\n",
    "    out = []\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    num_pixels = res**2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == num_pixels:\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[\n",
    "                    select\n",
    "                ]\n",
    "                out.append(cross_maps)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "\n",
    "def make_controller(\n",
    "    prompts: List[str],\n",
    "    is_replace_controller: bool,\n",
    "    cross_replace_steps: Dict[str, float],\n",
    "    self_replace_steps: float,\n",
    "    blend_words=None,\n",
    "    equilizer_params=None,\n",
    ") -> AttentionControlEdit:\n",
    "    if blend_words is None:\n",
    "        lb = None\n",
    "    else:\n",
    "        lb = LocalBlend(prompts, blend_word)\n",
    "    if is_replace_controller:\n",
    "        controller = AttentionReplace(\n",
    "            prompts,\n",
    "            NUM_DIFFUSION_STEPS,\n",
    "            cross_replace_steps=cross_replace_steps,\n",
    "            self_replace_steps=self_replace_steps,\n",
    "            local_blend=lb,\n",
    "        )\n",
    "    else:\n",
    "        controller = AttentionRefine(\n",
    "            prompts,\n",
    "            NUM_DIFFUSION_STEPS,\n",
    "            cross_replace_steps=cross_replace_steps,\n",
    "            self_replace_steps=self_replace_steps,\n",
    "            local_blend=lb,\n",
    "        )\n",
    "    if equilizer_params is not None:\n",
    "        eq = get_equalizer(\n",
    "            prompts[1], equilizer_params[\"words\"], equilizer_params[\"values\"]\n",
    "        )\n",
    "        controller = AttentionReweight(\n",
    "            prompts,\n",
    "            NUM_DIFFUSION_STEPS,\n",
    "            cross_replace_steps=cross_replace_steps,\n",
    "            self_replace_steps=self_replace_steps,\n",
    "            equalizer=eq,\n",
    "            local_blend=lb,\n",
    "            controller=controller,\n",
    "        )\n",
    "    return controller\n",
    "\n",
    "\n",
    "def show_cross_attention(\n",
    "    attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0\n",
    "):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.stack(images, axis=0))\n",
    "\n",
    "\n",
    "def show_self_attention_comp(\n",
    "    attention_store: AttentionStore,\n",
    "    res: int,\n",
    "    from_where: List[str],\n",
    "    max_com=10,\n",
    "    select: int = 0,\n",
    "):\n",
    "    attention_maps = (\n",
    "        aggregate_attention(attention_store, res, from_where, False, select)\n",
    "        .numpy()\n",
    "        .reshape((res**2, res**2))\n",
    "    )\n",
    "    u, s, vh = np.linalg.svd(\n",
    "        attention_maps - np.mean(attention_maps, axis=1, keepdims=True)\n",
    "    )\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.concatenate(images, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_512(image_or_path):\n",
    "    \"\"\"input PIL Image or image path, resize to 512x512\"\"\"\n",
    "    if isinstance(image_or_path, str):\n",
    "        image = np.array(Image.open(image_or_path))[:, :, :3]\n",
    "    elif isinstance(image_or_path, Image.Image):\n",
    "        image = np.array(image_or_path.convert(\"RGB\"))\n",
    "    else:\n",
    "        raise TypeError\n",
    "    image = np.array(Image.fromarray(image).resize((512, 512)))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERDDCI Inversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERDDCIInversion:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.scheduler = self.model.scheduler\n",
    "        self.scheduler.set_timesteps(NUM_DIFFUSION_STEPS)\n",
    "        self.prompt = None\n",
    "        self.context = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def latent2image(self, latents, return_type=\"np\"):\n",
    "        latents = 1 / 0.18215 * latents.detach()\n",
    "        image = self.model.vae.decode(latents)[\"sample\"]\n",
    "        if return_type == \"np\":\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def image2latent(self, image):\n",
    "        with torch.no_grad():\n",
    "            if type(image) is Image:\n",
    "                image = np.array(image)\n",
    "            if type(image) is torch.Tensor and image.dim() == 4:\n",
    "                latents = image\n",
    "            else:\n",
    "                image = torch.from_numpy(image).float() / 127.5 - 1\n",
    "                image = image.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "                latents = self.model.vae.encode(image)[\"latent_dist\"].mean\n",
    "                latents = latents * 0.18215\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_prompt(self, prompt: str):\n",
    "        uncond_input = self.model.tokenizer(\n",
    "            [\"\"],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        uncond_embeddings = self.model.text_encoder(\n",
    "            uncond_input.input_ids.to(self.model.device)\n",
    "        )[0]\n",
    "        text_input = self.model.tokenizer(\n",
    "            [prompt],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeddings = self.model.text_encoder(\n",
    "            text_input.input_ids.to(self.model.device)\n",
    "        )[0]\n",
    "        self.context = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        self.prompt = prompt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def prev_step(\n",
    "        self,\n",
    "        model_output: Union[torch.FloatTensor, np.ndarray],\n",
    "        timestep: int,\n",
    "        sample: Union[torch.FloatTensor, np.ndarray],\n",
    "    ):\n",
    "        prev_timestep = (\n",
    "            timestep\n",
    "            - self.scheduler.config.num_train_timesteps\n",
    "            // self.scheduler.num_inference_steps\n",
    "        )\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_prev = (\n",
    "            self.scheduler.alphas_cumprod[prev_timestep]\n",
    "            if prev_timestep >= 0\n",
    "            else self.scheduler.final_alpha_cumprod\n",
    "        )\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        pred_original_sample = (\n",
    "            sample - beta_prod_t**0.5 * model_output\n",
    "        ) / alpha_prod_t**0.5\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev) ** 0.5 * model_output\n",
    "        prev_sample = (\n",
    "            alpha_prod_t_prev**0.5 * pred_original_sample + pred_sample_direction\n",
    "        )\n",
    "        return prev_sample\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def next_step(\n",
    "        self,\n",
    "        model_output: Union[torch.FloatTensor, np.ndarray],\n",
    "        timestep: int,\n",
    "        sample: Union[torch.FloatTensor, np.ndarray],\n",
    "    ):\n",
    "        timestep, next_timestep = (\n",
    "            min(\n",
    "                timestep\n",
    "                - self.scheduler.config.num_train_timesteps\n",
    "                // self.scheduler.num_inference_steps,\n",
    "                999,\n",
    "            ),\n",
    "            timestep,\n",
    "        )\n",
    "        alpha_prod_t = (\n",
    "            self.scheduler.alphas_cumprod[timestep]\n",
    "            if timestep >= 0\n",
    "            else self.scheduler.final_alpha_cumprod\n",
    "        )\n",
    "        alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep]\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        next_original_sample = (\n",
    "            sample - beta_prod_t**0.5 * model_output\n",
    "        ) / alpha_prod_t**0.5\n",
    "        next_sample_direction = (1 - alpha_prod_t_next) ** 0.5 * model_output\n",
    "        next_sample = (\n",
    "            alpha_prod_t_next**0.5 * next_original_sample + next_sample_direction\n",
    "        )\n",
    "        return next_sample\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_noise_pred_invert(self, latents, t, context):\n",
    "        noise_pred = self.model.unet(latents, t, encoder_hidden_states=context)[\n",
    "            \"sample\"\n",
    "        ]\n",
    "        return noise_pred\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_noise_pred(\n",
    "        self, latents, t, guidance_scale, context=None, low_resource=LOW_RESOURCE\n",
    "    ):\n",
    "        if context is None:\n",
    "            context = self.context\n",
    "        uncond_embeddings, text_embeddings = context.chunk(2)\n",
    "        if low_resource:\n",
    "            noise_pred_uncond = self.model.unet(\n",
    "                latents, t, encoder_hidden_states=uncond_embeddings\n",
    "            )[\"sample\"]\n",
    "            noise_pred_cond = self.model.unet(\n",
    "                latents, t, encoder_hidden_states=text_embeddings\n",
    "            )[\"sample\"]\n",
    "        else:\n",
    "            latents_input = torch.cat([latents] * 2)\n",
    "            noise_pred = self.model.unet(\n",
    "                latents_input, t, encoder_hidden_states=context\n",
    "            )[\"sample\"]\n",
    "            noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (\n",
    "            noise_pred_cond - noise_pred_uncond\n",
    "        )\n",
    "\n",
    "        return noise_pred\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_loop(self, latent):\n",
    "        aux_latent_list = [latent.clone().detach()]\n",
    "        ori_latent_list = [latent.clone().detach()]\n",
    "        _, text_embeddings = self.context.chunk(2)\n",
    "        for i in range(NUM_DIFFUSION_STEPS):\n",
    "            t = self.model.scheduler.timesteps[\n",
    "                len(self.model.scheduler.timesteps) - i - 1\n",
    "            ]\n",
    "            ori_latent_t = ori_latent_list[-1]\n",
    "            # print(f\"[Debug] ddim_loop: {i=}\\t{t=}\")\n",
    "            noise_pred = self.get_noise_pred_invert(ori_latent_t, t, text_embeddings)\n",
    "            ori_latent_t_next = self.next_step(noise_pred, t, ori_latent_t)\n",
    "            ori_latent_list.append(ori_latent_t_next)\n",
    "\n",
    "            aux_latent_t = aux_latent_list[-1]\n",
    "            noise_pred = self.get_noise_pred_invert(\n",
    "                ori_latent_t_next, t, text_embeddings\n",
    "            )\n",
    "            aux_latent_t_next = self.next_step(noise_pred, t, aux_latent_t)\n",
    "            aux_latent_list.append(aux_latent_t_next)\n",
    "\n",
    "        return ori_latent_list, aux_latent_list\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_inversion(self, image):\n",
    "        latent = self.image2latent(image)\n",
    "        image_rec = self.latent2image(latent)\n",
    "        ori_latent_list, aux_latent_list = self.ddim_loop(latent)\n",
    "        return image_rec, ori_latent_list, aux_latent_list\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def invert(self, image_path, prompt: str, verbose=False):\n",
    "        self.init_prompt(prompt)\n",
    "        ptp_utils.register_attention_control(self.model, None)\n",
    "        image_gt = load_512(image_path)\n",
    "        if verbose:\n",
    "            print(\"ERDDCI inversion...\")\n",
    "\n",
    "        image_rec, ori_latent_list, aux_latent_list = self.ddim_inversion(image_gt)\n",
    "        return (image_gt, image_rec), ori_latent_list, aux_latent_list\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_encode_invert_inference_latent(\n",
    "        self, image_or_path, prompt: str, guidance_scale=1.0\n",
    "    ):\n",
    "        self.init_prompt(prompt)\n",
    "        ptp_utils.register_attention_control(self.model, None)\n",
    "        image_gt = load_512(image_or_path)\n",
    "        encode_latent = self.image2latent(image_gt)\n",
    "\n",
    "        ori_latent_list, aux_latent_list = self.ddim_loop(encode_latent)\n",
    "        ddim_invert_latent = ori_latent_list[-1]\n",
    "        erddci_invert_latent = aux_latent_list[-1]\n",
    "\n",
    "        ddim_inference_latent = ddim_invert_latent.clone().detach()\n",
    "        erddci_inference_latent = erddci_invert_latent.clone().detach()\n",
    "        reversed_ori_latent_list = list(reversed(ori_latent_list))\n",
    "        for i in range(NUM_DIFFUSION_STEPS):\n",
    "            t = self.model.scheduler.timesteps[i]\n",
    "            ori_latent_t = reversed_ori_latent_list[i]\n",
    "            # print(f\"[Debug] inference: {i=}\\t{t=}\")\n",
    "            noise_pred = self.get_noise_pred(\n",
    "                ori_latent_t, t, guidance_scale, self.context\n",
    "            )\n",
    "\n",
    "            ddim_inference_latent = self.prev_step(noise_pred, t, ddim_inference_latent)\n",
    "\n",
    "            erddci_inference_latent = self.prev_step(\n",
    "                noise_pred, t, erddci_inference_latent\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            encode_latent,\n",
    "            ddim_invert_latent,\n",
    "            erddci_invert_latent,\n",
    "            ddim_inference_latent,\n",
    "            erddci_inference_latent,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "erddci_inversion = ERDDCIInversion(ldm_stable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djm/ERDDCI/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/djm/ERDDCI/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/djm/ERDDCI/.venv/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def calculate_metrics(\n",
    "    original_image, reconstructed_image, LPIPS=lpips.LPIPS(net=\"vgg\").to(device)\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate LPIPS, PSNR, and SSIM Metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to PyTorch Tensor and Normalize to [-1, 1]\n",
    "    def to_tensor(img):\n",
    "        img_t = torch.tensor(img).float().permute(2, 0, 1) / 127.5 - 1\n",
    "        return img_t.unsqueeze(0).to(device)\n",
    "\n",
    "    original_tensor = to_tensor(original_image)\n",
    "    reconstructed_tensor = to_tensor(reconstructed_image)\n",
    "\n",
    "    lpips_value = LPIPS(original_tensor, reconstructed_tensor).item()\n",
    "\n",
    "    psnr_value = PSNR(original_image, reconstructed_image)\n",
    "\n",
    "    ssim_value = SSIM(\n",
    "        original_image, reconstructed_image, channel_axis=-1, data_range=255\n",
    "    )\n",
    "\n",
    "    return lpips_value, psnr_value, ssim_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the `image_folder` path as follows:\n",
    "\n",
    "- For the `wild-ti2i` dataset, use `real_path/to/wild-ti2i`\n",
    "- For the `imagenetr-ti2i` dataset, use `real_path/to/imagenetr-ti2i`\n",
    "\n",
    "For **DOCCI**, simply provide an empty directory named `DOCCI` – the actual image data will be automatically cached by the `datasets` package.\n",
    "\n",
    "The restructured results will also be saved in their corresponding folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output folder: ./ReconDataset/imagenetr-ti2i/recon_G1S50\n",
      "Dataset({\n",
      "    features: ['image', 'description'],\n",
      "    num_rows: 30\n",
      "})\n",
      "imagenetr-ti2i\n",
      "{'image': '/home/djm/ERDDCI/ReconDataset/imagenetr-ti2i/data/a cartoon of a castle.jpg', 'description': 'a cartoon of a castle'}\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"./ReconDataset/imagenetr-ti2i/\"\n",
    "output_folder = os.path.join(\n",
    "    image_folder, f\"recon_G{int(GUIDANCE_SCALE)}S{NUM_DIFFUSION_STEPS}\"\n",
    ")\n",
    "results_file = os.path.join(\n",
    "    output_folder, f\"erddci_results_G{int(GUIDANCE_SCALE)}S{NUM_DIFFUSION_STEPS}.txt\"\n",
    ")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    print(f\"Creating output folder: {output_folder}\")\n",
    "\n",
    "if \"DOCCI\" in image_folder:\n",
    "    recon_test_dataset = load_dataset(\"google/docci\", split=\"test\")\n",
    "else:\n",
    "    recon_test_dataset = load_dataset(\n",
    "        image_folder, split=\"test\", trust_remote_code=True\n",
    "    )\n",
    "\n",
    "print(recon_test_dataset)\n",
    "print(recon_test_dataset.info.dataset_name)\n",
    "print(recon_test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: a cartoon of a castle.jpg, prompt='a cartoon of a castle'\n",
      "a cartoon of a castle.jpg - DDIM: LPIPS=0.0443, PSNR=34.1093, SSIM=0.9735\n",
      "a cartoon of a castle.jpg - ERDDCI: LPIPS=0.0001, PSNR=69.5340, SSIM=1.0000\n",
      "  Reconstructed (GT) saved to: ./ReconDataset/imagenetr-ti2i/recon_G1S50/recon_encode_a cartoon of a castle.jpg\n",
      "  Reconstructed (DDIM) saved to: ./ReconDataset/imagenetr-ti2i/recon_G1S50/recon_ddim_a cartoon of a castle.jpg\n",
      "  Reconstructed (ERDDCI) saved to: ./ReconDataset/imagenetr-ti2i/recon_G1S50/recon_erddci_a cartoon of a castle.jpg\n",
      "Processing: a cartoon of a cat.jpg, prompt='a cartoon of a cat'\n",
      "a cartoon of a cat.jpg - DDIM: LPIPS=0.0518, PSNR=29.8792, SSIM=0.9384\n",
      "a cartoon of a cat.jpg - ERDDCI: LPIPS=0.0000, PSNR=65.7139, SSIM=1.0000\n",
      "  Reconstructed (GT) saved to: ./ReconDataset/imagenetr-ti2i/recon_G1S50/recon_encode_a cartoon of a cat.jpg\n",
      "  Reconstructed (DDIM) saved to: ./ReconDataset/imagenetr-ti2i/recon_G1S50/recon_ddim_a cartoon of a cat.jpg\n",
      "  Reconstructed (ERDDCI) saved to: ./ReconDataset/imagenetr-ti2i/recon_G1S50/recon_erddci_a cartoon of a cat.jpg\n",
      "Processing: a cartoon of a goldfish.jpg, prompt='a cartoon of a goldfish'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# invert and inference\u001b[39;00m\n\u001b[32m     30\u001b[39m     (\n\u001b[32m     31\u001b[39m         encode_latent,\n\u001b[32m     32\u001b[39m         ddim_invert_latent,\n\u001b[32m     33\u001b[39m         erddci_invert_latent,\n\u001b[32m     34\u001b[39m         ddim_inference_latent,\n\u001b[32m     35\u001b[39m         erddci_inference_latent,\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     ) = \u001b[43merddci_inversion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_encode_invert_inference_latent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGUIDANCE_SCALE\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[32m     41\u001b[39m     recon_image_encode = erddci_inversion.latent2image(encode_latent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ERDDCI/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 214\u001b[39m, in \u001b[36mERDDCIInversion.get_encode_invert_inference_latent\u001b[39m\u001b[34m(self, image_or_path, prompt, guidance_scale)\u001b[39m\n\u001b[32m    212\u001b[39m ori_latent_t = reversed_ori_latent_list[i]\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# print(f\"[Debug] inference: {i=}\\t{t=}\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m noise_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_noise_pred\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mori_latent_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m ddim_inference_latent = \u001b[38;5;28mself\u001b[39m.prev_step(noise_pred, t, ddim_inference_latent)\n\u001b[32m    220\u001b[39m erddci_inference_latent = \u001b[38;5;28mself\u001b[39m.prev_step(\n\u001b[32m    221\u001b[39m     noise_pred, t, erddci_inference_latent\n\u001b[32m    222\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ERDDCI/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 141\u001b[39m, in \u001b[36mERDDCIInversion.get_noise_pred\u001b[39m\u001b[34m(self, latents, t, guidance_scale, context, low_resource)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     latents_input = torch.cat([latents] * \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     noise_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatents_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33msample\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    144\u001b[39m     noise_pred_uncond, noise_pred_cond = noise_pred.chunk(\u001b[32m2\u001b[39m)\n\u001b[32m    146\u001b[39m noise_pred = noise_pred_uncond + guidance_scale * (\n\u001b[32m    147\u001b[39m     noise_pred_cond - noise_pred_uncond\n\u001b[32m    148\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ERDDCI/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ERDDCI/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ERDDCI/.venv/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_condition.py:1140\u001b[39m, in \u001b[36mUNet2DConditionModel.forward\u001b[39m\u001b[34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[39m\n\u001b[32m   1137\u001b[39m     sample = \u001b[32m2\u001b[39m * sample - \u001b[32m1.0\u001b[39m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# 1. time\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m t_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_time_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m emb = \u001b[38;5;28mself\u001b[39m.time_embedding(t_emb, timestep_cond)\n\u001b[32m   1143\u001b[39m class_emb = \u001b[38;5;28mself\u001b[39m.get_class_embed(sample=sample, class_labels=class_labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ERDDCI/.venv/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_condition.py:922\u001b[39m, in \u001b[36mUNet2DConditionModel.get_time_embed\u001b[39m\u001b[34m(self, sample, timestep)\u001b[39m\n\u001b[32m    920\u001b[39m     timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(timesteps.shape) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     timesteps = \u001b[43mtimesteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\u001b[39;00m\n\u001b[32m    925\u001b[39m timesteps = timesteps.expand(sample.shape[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results_data = []\n",
    "RECONSTRUCT_NUM_LIMIT = 500  # limit max reconstruct num\n",
    "loop_counter = 0\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(\n",
    "        f\"Reconstruction Test Results (Input Folder: {image_folder}, Output Folder: {output_folder}):\\n\"\n",
    "    )\n",
    "\n",
    "    for item in recon_test_dataset:\n",
    "        if loop_counter >= RECONSTRUCT_NUM_LIMIT:\n",
    "            break\n",
    "\n",
    "        image_or_path = item[\"image\"]\n",
    "        prompt = item[\"description\"]\n",
    "        if recon_test_dataset.info.dataset_name == \"docci\":\n",
    "            filename = f\"{item['example_id']}.png\"\n",
    "        else:\n",
    "            filename = os.path.basename(image_or_path)\n",
    "\n",
    "        print(f\"Processing: {filename}, {prompt=}\")\n",
    "        f.write(f\"Processing image: {filename}\\n\")\n",
    "\n",
    "        # Load image\n",
    "        original_image = load_512(image_or_path)\n",
    "        if original_image is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # invert and inference\n",
    "            (\n",
    "                encode_latent,\n",
    "                ddim_invert_latent,\n",
    "                erddci_invert_latent,\n",
    "                ddim_inference_latent,\n",
    "                erddci_inference_latent,\n",
    "            ) = erddci_inversion.get_encode_invert_inference_latent(\n",
    "                image_or_path, prompt, guidance_scale=GUIDANCE_SCALE\n",
    "            )\n",
    "\n",
    "            # Decode\n",
    "            recon_image_encode = erddci_inversion.latent2image(encode_latent)\n",
    "            recon_image_ddim = erddci_inversion.latent2image(ddim_inference_latent)\n",
    "            recon_image_erddci = erddci_inversion.latent2image(erddci_inference_latent)\n",
    "            # Calculate evaluation metrics\n",
    "            metrics_ddim = calculate_metrics(recon_image_encode, recon_image_ddim)\n",
    "            metrics_erddci = calculate_metrics(recon_image_encode, recon_image_erddci)\n",
    "\n",
    "            # Save result\n",
    "            results_data.append(\n",
    "                {\n",
    "                    \"filename\": filename,\n",
    "                    \"ddim_lpips\": metrics_ddim[0],\n",
    "                    \"ddim_psnr\": metrics_ddim[1],\n",
    "                    \"ddim_ssim\": metrics_ddim[2],\n",
    "                    \"erddci_lpips\": metrics_erddci[0],\n",
    "                    \"erddci_psnr\": metrics_erddci[1],\n",
    "                    \"erddci_ssim\": metrics_erddci[2],\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"{filename} - DDIM: LPIPS={metrics_ddim[0]:.4f}, PSNR={metrics_ddim[1]:.4f}, SSIM={metrics_ddim[2]:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"{filename} - ERDDCI: LPIPS={metrics_erddci[0]:.4f}, PSNR={metrics_erddci[1]:.4f}, SSIM={metrics_erddci[2]:.4f}\"\n",
    "            )\n",
    "            # Save recon images\n",
    "            output_image_encode = Image.fromarray(recon_image_encode)\n",
    "            output_path_encode = os.path.join(output_folder, f\"recon_encode_{filename}\")\n",
    "            output_image_encode.save(output_path_encode)\n",
    "            print(f\"  Reconstructed (GT) saved to: {output_path_encode}\")\n",
    "\n",
    "            output_image_ddim = Image.fromarray(recon_image_ddim)\n",
    "            output_path_ddim = os.path.join(output_folder, f\"recon_ddim_{filename}\")\n",
    "            output_image_ddim.save(output_path_ddim)\n",
    "            print(f\"  Reconstructed (DDIM) saved to: {output_path_ddim}\")\n",
    "\n",
    "            output_image_erddci = Image.fromarray(recon_image_erddci)\n",
    "            output_path_erddci = os.path.join(output_folder, f\"recon_erddci_{filename}\")\n",
    "            output_image_erddci.save(output_path_erddci)\n",
    "            print(f\"  Reconstructed (ERDDCI) saved to: {output_path_erddci}\")\n",
    "\n",
    "            loop_counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {filename}: {e}\")\n",
    "\n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    average_metrics = df_results[\n",
    "        [\n",
    "            \"ddim_lpips\",\n",
    "            \"ddim_psnr\",\n",
    "            \"ddim_ssim\",\n",
    "            \"erddci_lpips\",\n",
    "            \"erddci_psnr\",\n",
    "            \"erddci_ssim\",\n",
    "        ]\n",
    "    ].mean()\n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    print(average_metrics)\n",
    "\n",
    "    # 将 DataFrame 保存为 CSV 文件\n",
    "    csv_output_file = results_file.replace(\".txt\", \".csv\")\n",
    "    df_results.to_csv(csv_output_file, index=False)\n",
    "    print(f\"\\nResults saved to {csv_output_file}\")\n",
    "\n",
    "    json_output_file = results_file.replace(\".txt\", \".json\")\n",
    "    df_results.to_json(json_output_file, orient=\"records\", indent=4)\n",
    "    print(f\"Results saved to {json_output_file}\")\n",
    "\n",
    "print(\n",
    "    \"Reconstruction test completed! Results are saved in reconstruction_results.txt, and reconstructed images are in the corresponding '_recon' folder.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
